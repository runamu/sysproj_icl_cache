{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model_path=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# # Load the LLaMA 2 tokenizer (assuming it's already available via Hugging Face)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# # Sample text prompt\n",
    "# prompt = \"This is an example text to count the tokens.\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Count the number of tokens\n",
    "# token_count = tokens.shape[1]\n",
    "\n",
    "# print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/yedasong/sysproj_data/ShareGPT_V3_unfiltered_cleaned_split.json', 'r') as f:\n",
    "    sharegpt_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94145"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sharegpt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: You are trying to access a gated repo.\n",
      "Make sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from collections import Counter\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_token_frequencies(data: List[Dict], model_name: str = \"meta-llama/Llama-2-7b-hf\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize conversation data using Llama tokenizer and analyze token frequencies.\n",
    "\n",
    "    Args:\n",
    "        data: List of dictionaries containing conversation data\n",
    "        model_name: Name of the Llama model to use for tokenization\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with token frequencies sorted from highest to lowest\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Extract all conversation texts\n",
    "    all_texts = []\n",
    "    for item in data:\n",
    "        for conv in item['conversations']:\n",
    "            all_texts.append(conv['value'])\n",
    "\n",
    "    # Combine all texts\n",
    "    combined_text = \" \".join(all_texts)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(combined_text, add_special_tokens=True)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    token_strings = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "    # Count frequencies\n",
    "    token_frequencies = Counter(token_strings)\n",
    "\n",
    "    # Convert to DataFrame and sort\n",
    "    freq_df = pd.DataFrame.from_dict(token_frequencies, orient='index', columns=['frequency'])\n",
    "    freq_df.index.name = 'token'\n",
    "    freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "\n",
    "    # Add percentage column\n",
    "    total_tokens = freq_df['frequency'].sum()\n",
    "    freq_df['percentage'] = (freq_df['frequency'] / total_tokens * 100).round(2)\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "def save_frequency_analysis(freq_df: pd.DataFrame, output_path: str = \"token_frequencies.csv\"):\n",
    "    \"\"\"\n",
    "    Save the frequency analysis to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        freq_df: DataFrame containing token frequencies\n",
    "        output_path: Path to save the CSV file\n",
    "    \"\"\"\n",
    "    freq_df.to_csv(output_path)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    sample_data = [\n",
    "        {\n",
    "            'id': 'lDVDe5H_91',\n",
    "            'conversations': [\n",
    "                {'from': 'gpt', 'value': 'Hello, how can I help you today?'},\n",
    "                {'from': 'human', 'value': 'I need assistance with coding.'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Perform analysis\n",
    "    try:\n",
    "        freq_df = analyze_token_frequencies(sample_data)\n",
    "\n",
    "        # Display top 10 most frequent tokens\n",
    "        print(\"\\nTop 10 most frequent tokens:\")\n",
    "        print(freq_df.head(10))\n",
    "\n",
    "        # Save results\n",
    "        save_frequency_analysis(freq_df)\n",
    "        print(\"\\nResults saved to token_frequencies.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need acess to the llama model. Meanwhile, we can use gpt2 tokenizer that is freely available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 133kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 13.3MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 28.1MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 48.7MB/s]\n",
      "Downloading config.json: 100%|██████████| 665/665 [00:00<00:00, 1.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization Statistics:\n",
      "Total tokens: 15\n",
      "Unique tokens: 14\n",
      "Top 10 tokens cover: 73.36% of all tokens\n",
      "\n",
      "Top 10 most frequent tokens:\n",
      "       frequency  percentage  cumulative_percentage\n",
      "token                                              \n",
      "I              2       13.33                  13.33\n",
      "Hello          1        6.67                  20.00\n",
      ",              1        6.67                  26.67\n",
      "how            1        6.67                  33.34\n",
      "can            1        6.67                  40.01\n",
      "help           1        6.67                  46.68\n",
      "you            1        6.67                  53.35\n",
      "today          1        6.67                  60.02\n",
      "?              1        6.67                  66.69\n",
      "need           1        6.67                  73.36\n",
      "\n",
      "Results saved to token_frequencies.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from collections import Counter\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_token_frequencies(data: List[Dict], model_name: str = \"gpt2\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize conversation data using GPT-2 tokenizer and analyze token frequencies.\n",
    "\n",
    "    Args:\n",
    "        data: List of dictionaries containing conversation data\n",
    "        model_name: Name of the model to use for tokenization (default: \"gpt2\")\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with token frequencies sorted from highest to lowest\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Extract all conversation texts\n",
    "    all_texts = []\n",
    "    for item in data:\n",
    "        for conv in item['conversations']:\n",
    "            all_texts.append(conv['value'])\n",
    "\n",
    "    # Combine all texts\n",
    "    combined_text = \" \".join(all_texts)\n",
    "\n",
    "    # Tokenize the text\n",
    "    encoded = tokenizer.encode(combined_text, add_special_tokens=True)\n",
    "\n",
    "    # Convert token IDs back to tokens\n",
    "    token_strings = [tokenizer.decode([token]).strip() for token in encoded]\n",
    "\n",
    "    # Count frequencies\n",
    "    token_frequencies = Counter(token_strings)\n",
    "\n",
    "    # Convert to DataFrame and sort\n",
    "    freq_df = pd.DataFrame.from_dict(token_frequencies, orient='index', columns=['frequency'])\n",
    "    freq_df.index.name = 'token'\n",
    "    freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "\n",
    "    # Add percentage column\n",
    "    total_tokens = freq_df['frequency'].sum()\n",
    "    freq_df['percentage'] = (freq_df['frequency'] / total_tokens * 100).round(2)\n",
    "\n",
    "    # Add cumulative percentage\n",
    "    freq_df['cumulative_percentage'] = freq_df['percentage'].cumsum().round(2)\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "def save_frequency_analysis(freq_df: pd.DataFrame, output_path: str = \"token_frequencies.csv\"):\n",
    "    \"\"\"\n",
    "    Save the frequency analysis to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        freq_df: DataFrame containing token frequencies\n",
    "        output_path: Path to save the CSV file\n",
    "    \"\"\"\n",
    "    freq_df.to_csv(output_path)\n",
    "\n",
    "def print_token_statistics(freq_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print useful statistics about the tokenization.\n",
    "\n",
    "    Args:\n",
    "        freq_df: DataFrame containing token frequencies\n",
    "    \"\"\"\n",
    "    total_tokens = freq_df['frequency'].sum()\n",
    "    unique_tokens = len(freq_df)\n",
    "    top_10_coverage = freq_df['percentage'].head(10).sum()\n",
    "\n",
    "    print(f\"\\nTokenization Statistics:\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Unique tokens: {unique_tokens:,}\")\n",
    "    print(f\"Top 10 tokens cover: {top_10_coverage:.2f}% of all tokens\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    sample_data = [\n",
    "        {\n",
    "            'id': 'lDVDe5H_91',\n",
    "            'conversations': [\n",
    "                {'from': 'gpt', 'value': 'Hello, how can I help you today?'},\n",
    "                {'from': 'human', 'value': 'I need assistance with coding.'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Perform analysis\n",
    "        freq_df = analyze_token_frequencies(sample_data)\n",
    "\n",
    "        # Print statistics\n",
    "        print_token_statistics(freq_df)\n",
    "\n",
    "        # Display top 10 most frequent tokens\n",
    "        print(\"\\nTop 10 most frequent tokens:\")\n",
    "        print(freq_df.head(10))\n",
    "\n",
    "        # Save results\n",
    "        save_frequency_analysis(freq_df)\n",
    "        print(\"\\nResults saved to token_frequencies.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13499 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization Statistics:\n",
      "Total tokens: 13,499\n",
      "Unique tokens: 1,797\n",
      "Top 10 tokens cover: 28.57% of all tokens\n",
      "\n",
      "Top 10 most frequent tokens:\n",
      "       frequency  percentage  cumulative_percentage\n",
      "token                                              \n",
      "             604        4.47                   4.47\n",
      "the          590        4.37                   8.84\n",
      ",            540        4.00                  12.84\n",
      ".            475        3.52                  16.36\n",
      "and          415        3.07                  19.43\n",
      "of           380        2.82                  22.25\n",
      "to           280        2.07                  24.32\n",
      "a            224        1.66                  25.98\n",
      "\"            209        1.55                  27.53\n",
      "in           140        1.04                  28.57\n",
      "\n",
      "Results saved to token_frequencies.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Perform analysis\n",
    "    freq_df = analyze_token_frequencies(sharegpt_data[:10])\n",
    "\n",
    "    # Print statistics\n",
    "    print_token_statistics(freq_df)\n",
    "\n",
    "    # Display top 10 most frequent tokens\n",
    "    print(\"\\nTop 10 most frequent tokens:\")\n",
    "    print(freq_df.head(10))\n",
    "\n",
    "    # Save results\n",
    "    save_frequency_analysis(freq_df)\n",
    "    print(\"\\nResults saved to token_frequencies.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "riddle_sense = load_dataset(\"bigbench\", 'riddle_sense')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All tasknames:\n",
    "['abstract_narrative_understanding', 'anachronisms', 'analogical_similarity', 'analytic_entailment', 'arithmetic', 'ascii_word_recognition', 'authorship_verification', 'auto_categorization', 'auto_debugging', 'bbq_lite_json', 'bridging_anaphora_resolution_barqa', 'causal_judgment', 'cause_and_effect', 'checkmate_in_one', 'chess_state_tracking', 'chinese_remainder_theorem', 'cifar10_classification', 'code_line_description', 'codenames', 'color', 'common_morpheme', 'conceptual_combinations', 'conlang_translation', 'contextual_parametric_knowledge_conflicts', 'crash_blossom', 'crass_ai', 'cryobiology_spanish', 'cryptonite', 'cs_algorithms', 'dark_humor_detection', 'date_understanding', 'disambiguation_qa', 'discourse_marker_prediction', 'disfl_qa', 'dyck_languages', 'elementary_math_qa', 'emoji_movie', 'emojis_emotion_prediction', 'empirical_judgments', 'english_proverbs', 'english_russian_proverbs', 'entailed_polarity', 'entailed_polarity_hindi', 'epistemic_reasoning', 'evaluating_information_essentiality', 'fact_checker', 'fantasy_reasoning', 'few_shot_nlg', 'figure_of_speech_detection', 'formal_fallacies_syllogisms_negation', 'gem', 'gender_inclusive_sentences_german', 'general_knowledge', 'geometric_shapes', 'goal_step_wikihow', 'gre_reading_comprehension', 'hhh_alignment', 'hindi_question_answering', 'hindu_knowledge', 'hinglish_toxicity', 'human_organs_senses', 'hyperbaton', 'identify_math_theorems', 'identify_odd_metaphor', 'implicatures', 'implicit_relations', 'intent_recognition', 'international_phonetic_alphabet_nli', 'international_phonetic_alphabet_transliterate', 'intersect_geometry', 'irony_identification', 'kanji_ascii', 'kannada', 'key_value_maps', 'known_unknowns', 'language_games', 'language_identification', 'linguistic_mappings', 'linguistics_puzzles', 'list_functions', 'logic_grid_puzzle', 'logical_args', 'logical_deduction', 'logical_fallacy_detection', 'logical_sequence', 'mathematical_induction', 'matrixshapes', 'metaphor_boolean', 'metaphor_understanding', 'minute_mysteries_qa', 'misconceptions', 'misconceptions_russian', 'mnist_ascii', 'modified_arithmetic', 'moral_permissibility', 'movie_dialog_same_or_different', 'movie_recommendation', 'mult_data_wrangling', 'multiemo', 'natural_instructions', 'navigate', 'nonsense_words_grammar', 'novel_concepts', 'object_counting', 'odd_one_out', 'operators', 'paragraph_segmentation', 'parsinlu_qa', 'parsinlu_reading_comprehension', 'penguins_in_a_table', 'periodic_elements', 'persian_idioms', 'phrase_relatedness', 'physical_intuition', 'physics', 'physics_questions', 'play_dialog_same_or_different', 'polish_sequence_labeling', 'presuppositions_as_nli', 'qa_wikidata', 'question_selection', 'real_or_fake_text', 'reasoning_about_colored_objects', 'repeat_copy_logic', 'rephrase', 'riddle_sense', 'ruin_names', 'salient_translation_error_detection', 'scientific_press_release', 'semantic_parsing_in_context_sparc', 'semantic_parsing_spider', 'sentence_ambiguity', 'similarities_abstraction', 'simp_turing_concept', 'simple_arithmetic_json', 'simple_arithmetic_json_multiple_choice', 'simple_arithmetic_json_subtasks', 'simple_arithmetic_multiple_targets_json', 'simple_ethical_questions', 'simple_text_editing', 'snarks', 'social_iqa', 'social_support', 'sports_understanding', 'strange_stories', 'strategyqa', 'sufficient_information', 'suicide_risk', 'swahili_english_proverbs', 'swedish_to_german_proverbs', 'symbol_interpretation', 'temporal_sequences', 'tense', 'timedial', 'topical_chat', 'tracking_shuffled_objects', 'understanding_fables', 'undo_permutation', 'unit_conversion', 'unit_interpretation', 'unnatural_in_context_learning', 'vitaminc_fact_verification', 'what_is_the_tao', 'which_wiki_edit', 'winowhy', 'word_sorting', 'word_unscrambling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_riddlesense_tokens(data: List[Dict], analyze_fields: List[str] = ['inputs', 'targets'],\n",
    "                             model_name: str = \"gpt2\") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Analyze token frequencies in RiddleSense dataset fields.\n",
    "\n",
    "    Args:\n",
    "        data: List of RiddleSense data entries\n",
    "        analyze_fields: Fields to analyze (default: ['inputs', 'targets'])\n",
    "        model_name: Name of the tokenizer model to use\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing frequency DataFrames for each analyzed field\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    results = {}\n",
    "\n",
    "    for field in analyze_fields:\n",
    "        # Collect all text from the specified field\n",
    "        all_texts = []\n",
    "        for item in data:\n",
    "            if field == 'targets':\n",
    "                # Handle targets which is a list\n",
    "                text = ' '.join(item[field])\n",
    "                all_texts.append(text)\n",
    "            else:\n",
    "                # Handle regular string fields\n",
    "                all_texts.append(item[field])\n",
    "\n",
    "        # Combine all texts\n",
    "        combined_text = \" \".join(all_texts)\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = tokenizer.encode(combined_text, add_special_tokens=True)\n",
    "        token_strings = [tokenizer.decode([token]).strip() for token in encoded]\n",
    "\n",
    "        # Count frequencies\n",
    "        token_frequencies = Counter(token_strings)\n",
    "\n",
    "        # Create DataFrame\n",
    "        freq_df = pd.DataFrame.from_dict(token_frequencies, orient='index', columns=['frequency'])\n",
    "        freq_df.index.name = 'token'\n",
    "        freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "\n",
    "        # Add percentage and cumulative percentage\n",
    "        total_tokens = freq_df['frequency'].sum()\n",
    "        freq_df['percentage'] = (freq_df['frequency'] / total_tokens * 100).round(2)\n",
    "        freq_df['cumulative_percentage'] = freq_df['percentage'].cumsum().round(2)\n",
    "\n",
    "        results[field] = freq_df\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_field_statistics(field_name: str, freq_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print statistics for a specific field's tokenization.\n",
    "\n",
    "    Args:\n",
    "        field_name: Name of the field being analyzed\n",
    "        freq_df: DataFrame containing token frequencies\n",
    "    \"\"\"\n",
    "    total_tokens = freq_df['frequency'].sum()\n",
    "    unique_tokens = len(freq_df)\n",
    "    top_10_coverage = freq_df['percentage'].head(10).sum()\n",
    "\n",
    "    print(f\"\\n{field_name.upper()} Field Statistics:\")\n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Unique tokens: {unique_tokens:,}\")\n",
    "    print(f\"Top 10 tokens cover: {top_10_coverage:.2f}% of all tokens\")\n",
    "    print(\"\\nTop 10 most frequent tokens:\")\n",
    "    print(freq_df[['frequency', 'percentage']].head(10))\n",
    "\n",
    "def save_analysis_results(results: Dict[str, pd.DataFrame], base_filename: str = \"riddlesense_tokens\"):\n",
    "    \"\"\"\n",
    "    Save analysis results to CSV files.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary of DataFrames containing frequency analysis\n",
    "        base_filename: Base name for the output files\n",
    "    \"\"\"\n",
    "    for field, df in results.items():\n",
    "        filename = f\"{base_filename}_{field}.csv\"\n",
    "        df.to_csv(filename)\n",
    "        print(f\"Saved {field} analysis to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample RiddleSense data\n",
    "    sample_data = [\n",
    "        {\n",
    "            'idx': 0,\n",
    "            'inputs': 'Q: My eyes are black and my hair is bright, and my feet are firmly rooted on the ground. I enjoy having the sun on my face, and I try to keep up with it. It is said that when I am dead and gone, I will droop real low, keeping the birds well fed, standing stiff in my row. What exactly am I?\\n  choice: corpse\\n  choice: sunflower\\n  choice: graveyard\\n  choice: bean\\n  choice: sunburns\\nA:',\n",
    "            'targets': ['sunflower'],\n",
    "            'multiple_choice_targets': ['bean', 'corpse', 'graveyard', 'sunflower', 'sunburns'],\n",
    "            'multiple_choice_scores': [0, 0, 0, 1, 0]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Perform analysis\n",
    "        results = analyze_riddlesense_tokens(sample_data)\n",
    "\n",
    "        # Print statistics for each analyzed field\n",
    "        for field_name, freq_df in results.items():\n",
    "            print_field_statistics(field_name, freq_df)\n",
    "\n",
    "        # Save results\n",
    "        save_analysis_results(results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = []\n",
    "for item in riddle_sense['default']:\n",
    "    sample_data.append(item)\n",
    "for item in riddle_sense['train']:\n",
    "    sample_data.append(item)\n",
    "for item in riddle_sense['validation']:\n",
    "    sample_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Perform analysis\n",
    "    results = analyze_riddlesense_tokens(sample_data)\n",
    "\n",
    "    # Print statistics for each analyzed field\n",
    "    for field_name, freq_df in results.items():\n",
    "        print_field_statistics(field_name, freq_df)\n",
    "\n",
    "    # Save results\n",
    "    save_analysis_results(results)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sysproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
